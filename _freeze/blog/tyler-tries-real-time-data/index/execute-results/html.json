{
  "hash": "875489e6b24cf0dea20ac949b0058e53",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tyler Tries Real-Time Data\"\ndescription: My experience building a real-time data pipeline to visualize Coinbase order book depth, highlighting the seamless integration of Redpanda, Materialize, dbt, and Streamlit.\nauthor: \"Tyler Hillery\"\ndate: \"2023-10-01\"\ncategories: [Real-Time Data, Data Apps, Data Engineering]\nimage: \"../../assets/images/streamlit_app_coinbase.png\"\nfilters:\n  - social-share\nexecute:\n  eval: false\nshare:\n  permalink: \"https://tylerhillery.com/blog/tyler-tries-real-time-data/\"\n  description:\n  twitter: true\n  facebook: false\n  reddit: true\n  stumble: false\n  tumblr: false\n  linkedin: true\n  email: true\n  mastodon: true\n---\n\n------------------------------------------------------------------------\n\n## Introducing \"Tyler Tries\"\n\nThis blog kicks off a new series I am calling \"Tyler Tries\" where I write about the topics I am learning about. We are going to start the series off with my experience building a real-time data pipeline to visualize Coinbase order book depth.\n\n## TL;DR\n\nI built a real-time dashboard that visualizes Coinbase order book depth powered by [Redpanda](https://redpanda.com/) + [Materialize](https://materialize.com/) + [dbt](https://www.getdbt.com/) + [Streamlit](https://streamlit.io/) you can view the code [here](https://github.com/TylerHillery/coinbase-order-book-pipeline). The data comes from the free Coinbase WebSocket feed using the [level2_batch channel](https://docs.cloud.coinbase.com/exchange/docs/websocket-channels#level2-batch-channel).\n\n<iframe src=\"https://drive.google.com/file/d/1yCz9NxpN5zCXPRSVosNWmkhWPnQeniP6/preview\" width=\"100%\" height=\"480\" allow=\"autoplay\">\n\n</iframe>\n\n## Introduction\n\nI posted the above video on Twitter/X and several people requested a blog post about how it all works. It may come as a bit of a surprise but it wasn't all too difficult. This is coming from someone without any prior experience dealing with real-time data and who has never used Redpanda or Materialize before. Funny enough, the part I struggled with most was pandas dataframe styling and creating an unstacked area chart (it turns out most python plotting libraries assume your area chart will be stacked).\n\nHistorically, real-time data was hard to manage. We've come a long way since then. If you look at the various components of the [modern streaming stack](https://medium.com/event-driven-utopia/unbundling-the-modern-streaming-stack-451f75eaf1d) the biggest thing slowing people down from building real-time data pipelines are the sources. Besides CDC feeds, very rarely are there [event-driven APIs](https://medium.com/event-driven-utopia/event-driven-apis-understanding-the-principles-c3208308d4b2) that allow for the consumption of the source data in real time.\n\nThis is why I got so excited when I came across this guide from bytewax: [Real-Time Financial Exchange Order Book](https://bytewax.io/guides/real-time-financial-exchange-order-book-application?utm_source=pocket_saves). It's how I discovered the free Coinbase WebSocket feed which displays their level 2 order book data. Using this data I thought it would be fun to recreate a common visual to display exchange depth.\n\n::: {style=\"font-size: 50%; text-align:center\"}\n##### **Coinbase Pro**\n\n![](../../assets/images/coinbasepro.png)\n\n##### **Robinhood**\n\n![](../../assets/images/robinhood.png)\n\n##### **Kraken**\n\n![](../../assets/images/kraken.png)\n:::\n\nThese visuals illustrate the cumulative size, indicating the quantity of a security someone is willing to buy or sell at a specific price level. This information is valuable as it can address questions such as, \"*At what price should I place my order to ensure it gets fully filled if I aim to buy or sell 100 quantities*\"\n\nThe pipeline will work as follows:\n\n1.  Create a `KafkaProducer` to send the data from the Coinbase WebSocket and add it to a Redpanda topic.\n2.  Ingest the Redpanda topic into Materialize using dbt to manage the transformations.\n3.  Connect Streamlit to Materialize with `psycopg2` to visualize the data.\n\n## Ingestion\n\nThe first step in this pipeline involves creating a `KafkaProducer` that will take the data from the Coinbase WebSocket and add it to a Redpanda topic.\n\nTo start ingesting data from Coinbase we submit an initial request to the WebSocket channel like so:\n\n``` python\n\nimport json\nfrom websocket import create_connection\nws = create_connection(\"wss://ws-feed.exchange.coinbase.com\")\nws.send(\n  json.dumps({\n    \"type\": \"subscribe\",\n    \"product_ids\": [\n        \"ETH-USD\",\n        \"BTC-USD\"\n    ],\n    \"channels\": [\"level2_batch\"]\n  })\n)\n```\n\nIf successful the first message back after running `print(ws.recv())` will look like:\n\n``` json\n{\n   \"type\":\"subscriptions\",\n   \"channels\":[\n      {\n         \"name\":\"level2_50\",\n         \"product_ids\":[\n            \"BTC-USD\",\n            \"ETH-USD\"\n         ]\n      }\n   ]\n}\n```\n\nFollowing the `subscriptions` message, the next message received will be the `snapshot` which provides an overview of the level 2 order book.\n\n``` json\n[\n   {\n      \"type\":\"snapshot\",\n      \"product_id\":\"BTC-USD\",\n      \"bids\":[\n         [\"10101.10\", \"0.45054140\"]\n      ],\n      \"asks\":[\n         [\"10102.55\",\"0.57753524\"]\n      ]\n   },\n   {\n      \"type\":\"snapshot\",\n      \"product_id\":\"ETH-USD\",\n      \"bids\":[\n         [\"10101.10\", \"0.45054140\"]\n      ],\n      \"asks\":[\n         [\"10102.55\",\"0.57753524\"]\n      ]\n   }\n]\n```\n\nThe `bids` and `asks` provide a list of lists where the first element is the price and the second element is the quantity.\n\nAfter the snapshot messages, the last message is `l2update` which provides a message if the `size` has changed at a given price level for the product_id.\n\n::: callout-note\nThe size property is the updated size at the price level, not a delta. A size of \"0\" indicates the price level can be removed.\n:::\n\n``` json\n{\n   \"type\":\"l2update\",\n   \"product_id\":\"BTC-USD\",\n   \"time\":\"2019-08-14T20:42:27.265Z\",\n   \"changes\":[\n      [\"buy\",\"10101.80000000\",\"0.162567\"]\n   ]\n}\n```\n\nTo process these messages within the `KafkaProducer` I've created an infinite while loop that will keep running until the program is stopped.\n\n::: {#b3f4185a .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code\"}\nwhile True:\n  message = ws.recv()\n  data = json.loads(message)\n  if data[\"type\"] == \"snapshot\":\n      \n      asks = [{\n              \"message_type\": \"snapshot\",\n              \"message_key\": data[\"product_id\"] + \"-sell-\" + str(order[0]),\n              \"product_id\": data[\"product_id\"],\n              \"side\": \"sell\",\n              \"price\": order[0],\n              \"size\": order[1],\n              \"message_created_at_utc\": format_datetime(data[\"time\"])\n              } for order in data[\"asks\"]\n          ]\n      \n      bids = [{\n              \"message_type\": \"snapshot\",\n              \"message_key\": data[\"product_id\"] + \"-buy-\" + str(order[0]),\n              \"product_id\": data[\"product_id\"],\n              \"side\": \"buy\",\n              \"price\": order[0],\n              \"size\": order[1],\n              \"message_created_at_utc\": format_datetime(data[\"time\"])\n              } for order in data[\"bids\"]\n          ]\n      \n      order_book = asks + bids\n\n      for order in order_book:\n          prod.send(\n              topic=\"coinbase_level2_channel\", \n              key=order[\"message_key\"].encode(\"utf-8\"),\n              value=json.dumps(order,default=json_serializer,ensure_ascii=False).encode(\"utf-8\")\n          )\n          print(order) #log\n      prod.flush()\n\n  elif data[\"type\"] == \"l2update\":\n      orders = [{\n              \"message_type\": \"l2update\",\n              \"message_key\": data[\"product_id\"] + \"-\" + order[0] + \"-\" + str(order[1]),\n              \"product_id\": data[\"product_id\"],\n              \"side\": order[0],\n              \"price\": order[1],\n              \"size\": order[2],\n              \"message_created_at_utc\": format_datetime(data[\"time\"])\n              } for order in data[\"changes\"]\n          ]\n      for order in orders:\n          prod.send(\n                  topic=\"coinbase_level2_channel\", \n                  key=order[\"message_key\"].encode(\"utf-8\"),\n                  value=json.dumps(order,default=json_serializer,ensure_ascii=False).encode(\"utf-8\")\n              )\n          print(order) #log\n      prod.flush()\n  else:\n      print(f\"Unexpected value for 'type': {data['type']}\")\n```\n:::\n\n\n## Redpanda Console\n\nThe Redpanda Console provided a nice UI to peer into how the Redpanda instance is operating. If you haven't used the Redpanda Console, it's described as:\n\n> **A Kafka web UI for developers**.\\\n> Redpanda Console gives you a simple, interactive approach for gaining visibility into your topics, masking data, managing consumer groups, and exploring real-time data with time-travel debugging[^1].\n\n![](../../assets/images/redpanda_console.png)\n\nThe console helped me identify a bug in my original code. The unique key I was making for each record was a combination of the product_id, side, & price e.g. `BTC-USD-buy-10000`. The problem was for the `snapshot` messages I was using the terms `bid` and `ask` but for the `l2update` messages I was using `buy` and `sell`. This was important to fix because the data was being inserted into Materialize via the [ENVELOPE UPSERT](https://materialize.com/docs/sql/create-source/kafka/#handling-upserts) based on this key.\n\n## Materialize & dbt\n\nThe next step in our pipeline entails processing and storing the real-time data. Materialize is a streaming database that allows for just that. It has integrations with Kafka and since Redpanda is compatible with Kafka APIs, Materialize and Redpanda work together out of the box.\n\nA key thing to understand about Materialize is how it handles materialized views. Their MVs are incrementally maintained so as the underlying data changes the MV automatically updates. This is why when you use Materialize and dbt together you'll typically set up dbt to only run in a CI/CD pipeline which is kicked off when changes occur to the dbt models. If you want to learn more about how this works I highly recommend checking out this video on [Materialize+dbt Streaming for the Modern Data Stack](https://youtu.be/yofTRM9WpwQ?si=9fdqDtII_GbH5dxG.).\n\nMost of the code for this project mimics what I found in the [Materialize + Redpanda + dbt Hack Day](https://github.com/MaterializeInc/mz-hack-day-2022). \n\nFirst we define our source:\n``` sql\n{{ config(materialized='source') }}\nCREATE SOURCE {{ this }}\nFROM KAFKA BROKER 'redpanda:9092' TOPIC 'coinbase_level2_channel'\n  KEY FORMAT BYTES\n  VALUE FORMAT BYTES\nENVELOPE UPSERT;\n```\n\nThe `ENVELOPE UPSERT` treats all records as having a key and a value, and supports inserts, updates and deletes within Materialize[^2]:\n\n- If the key does not match a preexisting record, it inserts the record’s key and value.\n- If the key matches a preexisting record and the value is non-null, Materialize updates the existing record with the new value.\n- If the key matches a preexisting record and the value is null, Materialize deletes the record.\n\nThe staging model does some light transformations to get the data into a more usable form.\n\n::: {#03286495 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"stg_coinbase_level2_channel\"}\n{{ \n    config(\n        materialized='materializedview'\n    ) \n}}\n\nwith \nsource as (\n    select * from {{ source('coinbase', 'level2_channel') }}\n),\n\nconverted as (\n    select convert_from(data, 'utf8') as data from source\n),\n\ncasted AS (\n    select cast(data as jsonb) as data from converted\n),\n\nrenamed as (\n    select\n        (data->>'message_type')::string                 as message_type,\n        (data->>'message_key')::string                  as message_key,\n        (data->>'product_id')::string                   as product_id,\n        (data->>'side')::string                         as side,\n        (data->>'price')::double                        as price,\n        (data->>'size')::double                         as size,\n        (data->>'message_created_at_utc')::timestamp    as message_created_at_utc\n    from\n        casted\n),\n\nfinal as (\n    select\n        message_type,\n        message_key,\n        product_id,\n        side,\n        price,\n        size,\n        price * size as notional_size,\n        message_created_at_utc\n    from\n        renamed\n    where\n        size != 0\n)\n\nselect * from final\n```\n:::\n\n\nBecause Materialize does not handle [window functions](https://materialize.com/docs/transform-data/patterns/window-functions/#:~:text=Window%20functions%20compute%20results%20based,consecutive%20rows%20in%20each%20partition.) very well they provided some alternative approaches in their docs. I referenced the [Materialize Top K by group](https://materialize.com/docs/transform-data/patterns/top-k/#top-1-using-distinct-on) that allowed me to return the top bid and ask record for each product id. The highest bid and lowest ask is referred to as the national best bid and offer (NBBO).\n\n::: {#44fec5d5 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"int_coinbase_nbbo\"}\n{{ \n    config(\n        materialized='materializedview'\n    ) \n}}\n\nwith\nstg_coinbase_level2_channel as (\n    select * from {{ ref('stg_coinbase_level2_channel') }}\n),\n\nnbb as (\n    select\n        distinct on(product_id) product_id,\n        side,\n        price,\n        size,\n        notional_size,\n        message_created_at_utc\n    from \n        stg_coinbase_level2_channel\n    where\n        side = 'buy'\n    order by\n        product_id, price desc\n),\n\nnbo as (\n    select\n        distinct on(product_id) product_id,\n        side,\n        price,\n        size,\n        notional_size,\n        message_created_at_utc\n    from \n        stg_coinbase_level2_channel\n    where\n        side = 'sell'\n    order by\n        product_id, price asc\n),\n\nunioned as (\n    select * from nbb\n    union all \n    select * from nbo\n)\n\nselect * from unioned\n```\n:::\n\n\nThe last model I created was the fct_coinbase_nbbo which pivots the NBB and NBO so there is only one record per product_id. This allows for the calculation of the NBBO spread and NBBO midpoint. The spread is the difference between the NBB and NBO. The NBBO midpoint is usually used as the reference price for the current market value of a security.\n\n::: {#dc244550 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"fct_coinbase_nbbo\"}\n{{ \n    config(\n        materialized='materializedview'\n    ) \n}}\n\nwith\nint_coinbase_nbbo as (\n    select * from {{ ref('int_coinbase_nbbo') }}\n),\n\nnbbo as (\n    select\n        product_id,\n        max(case when side = 'buy' then price end)                      as nbb_price,\n        max(case when side = 'buy' then size end)                       as nbb_size,\n        max(case when side = 'buy' then notional_size end)              as nbb_notional_size,\n        max(case when side = 'buy' then message_created_at_utc end)     as nbb_last_updated_at_utc,\n        max(case when side = 'sell' then price end)                     as nbo_price,\n        max(case when side = 'sell' then size end)                      as nbo_size,\n        max(case when side = 'sell' then notional_size end)             as nbo_notional_size,\n        max(case when side = 'sell' then message_created_at_utc end)    as nbo_last_updated_at_utc\n    from\n        int_coinbase_nbbo\n    group by\n        product_id\n),\n\nfinal as (\n    select\n        product_id,\n        nbb_price,\n        nbb_size,\n        nbb_notional_size,\n        nbb_last_updated_at_utc,\n        nbo_price,\n        nbo_size,\n        nbo_notional_size,\n        nbo_last_updated_at_utc,\n        (nbb_price + nbo_price) / 2 as nbbo_midpoint,\n        nbo_price - nbb_price as nbbo_spread\n    from\n        nbbo\n)\n\nselect * from final\n```\n:::\n\n\n**TODO**: Something I want to work on in the future is creating a model that provides the cumulative size. Normally a window function is what I would use to get the cumulative sum of something but window functions don't work well with Materialize. My theory is that I can do a self-join on the product_id, side, and price where the price is \\>= the current price (or \\<= depending on the side) and then sum up the size.\n\n## Streamlit\n\nStreamlit has been my go-to lately when I want to display a side project I've been working on. Because Materialize speaks the Postgres wire protocol I can leverage `psycopg2` to connect to Streamlit.\n\n``` python\nimport psycopg2\ndsn = \"user=materialize password=password host=materialized port=6875 dbname=materialize\"\nconn = psycopg2.connect(dsn)\ncur = conn.cursor()\n```\n\nCurrently, I am using a polling technique based on the `refresh_interval` that is set by the `st.slider` and defaults to 1 second. I use a package called `streamlit_autorefresh` that handles refreshing the Streamlit app.\n\n``` python\nimport streamlit as st\nfrom streamlit_autorefresh import st_autorefresh\n\nwith st.sidebar:\n    refresh_interval = st.slider(\n        \"Auto-Refresh Interval (seconds)\", \n        1, \n        15, \n        step=1\n    )\n\nst_autorefresh(interval=refresh_interval*1000)\n```\n\n**TODO**: I would like to switch this polling technique to a push model so the data is sent directly to Streamlit as it comes in. Shout-out to the Materialize team for whipping up a [demo](https://github.com/MaterializeInc/demos/pull/88) for me on how to do this!\n\n## Conclusion\nIncreasingly, individuals and organizations are discovering use cases for analytical data that have different standards than traditional OLAP data workflows, often venturing into what's known as \"Operational Analytics.\" Coming from an ops background, I’ve seen firsthand how operational processes can get caught up in mundane data tasks. This is where data teams can step in, alleviating this burden, allowing focus to shift to higher-value problems.\n\nThe evolution of tooling and technology surrounding real-time data has significantly eased the prior complexity. In the past, the cost and effort required to establish the necessary infrastructure were deterrents. However, times have changed. I firmly believe real-time isn't just the future; it's the present.\n\n### Other TODOs\n\n-   Utilize the Materialize [SUBSCRIBE](https://materialize.com/docs/sql/subscribe/) along with materializing `dbt tests` as MVs to create alerts on:\n    -   NBBO spread gets too wide\n    -   Price alerts\n-   If the change of the size for a specific price level goes down we can use it as a proxy for executed volume and display those records as a table.\n\n### References\n\n-   [Real-Time Financial Exchange Order Book](https://bytewax.io/guides/real-time-financial-exchange-order-book-application?utm_source=pocket_saves)\n-   [Streaming Data Apps with Bytewax and Streamlit](https://bytewax.io/blog/streaming-data-apps-with-bytewax-and-streamlit?utm_source=pocket_saves)\n-   [How to Build a Real-Time Feature Pipeline In Python](https://www.realworldmcl.xyz/blog/real-time-pipelines-in-python)\n-   [Temporal analysis of Wikipedia changes with Redpanda & Materialize & dbt](https://medium.com/@danthelion/temporal-analysis-of-wikipedia-changes-with-redpanda-materialize-dbt-e372186fb951)\n-   [Breathing life into Streamlit with Materialize & Redpanda](https://medium.com/@danthelion/breathing-life-into-streamlit-with-materialize-redpanda-1c29282cc72b)\n-   [How to build a real-time crypto tracker with Redpanda and QuestDB](https://redpanda.com/blog/real-time-crypto-tracker-questdb-redpanda)\n-   [Online Machine Learning in Practice: Interactive dashboards to detect data anomalies in real time](https://bytewax.io/blog/online-machine-learning-in-practice-interactive-dashboards-to-detect-data-anomalies-in-real-time)\n-   [Materialize Top K by group](https://materialize.com/docs/transform-data/patterns/top-k/#top-1-using-distinct-on)\n-   [Materialize + Redpanda + dbt Hack Day](https://github.com/MaterializeInc/mz-hack-day-2022)\n-   [Pandas Table Viz](https://pandas.pydata.org/docs/user_guide/style.html#Table-Styles)\n-   [Plotly Filled Area Plots](https://plotly.com/python/filled-area-plots/)\n-   [Deploy Streamlit using Docker](https://docs.streamlit.io/knowledge-base/tutorials/deploy/docker)\n\n[^1]: [Redpanda Console](https://redpanda.com/redpanda-console-kafka-ui)\n[^2]: [CREATE SOURCE | Materialize Docs](https://materialize.com/docs/sql/create-source/#:~:text=The%20upsert%20envelope%20treats%20all,the%20record's%20key%20and%20value.)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}